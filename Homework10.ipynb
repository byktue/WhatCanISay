{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据建模在机器学习中具有极其重要的地位。它是机器学习的核心组成部分，对于训练和评估模型、做出预测和优化决策都至关重要。\n",
    "\n",
    "模型训练和学习：在机器学习中，模型通过从数据中学习模式和关系来进行训练。模型的性能和准确性取决于所用数据的质量和数量。良好的数据建模可以提供高质量的训练数据，有助于构建更准确和可靠的模型。\n",
    "\n",
    "**机器学习的一般框架**\n",
    "1. 选择模型： 根据问题的性质，选择适当的机器学习模型。例如，对于分类问题，可以选择支持向量机、决策树、随机森林等。\n",
    "2. 划分数据集： 将数据集分为训练集和测试集，以便评估模型的性能。通常，80%的数据用于训练，20%用于测试。\n",
    "3. 训练模型： 使用训练数据集来拟合模型。\n",
    "4. 评估模型： 使用测试数据集评估模型性能。\n",
    "5. 调优模型： 根据模型性能进行调优，可能需要调整模型超参数、使用交叉验证等。\n",
    "6. 预测： 使用训练好的模型进行新数据的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 题目一、以鸢尾花数据集为例学习 SVM \n",
    "1. 加载数据，划分鸢尾花数据集，训练集比例0.2，随机种子42\n",
    "2. 创建并训练 SVM 模型，使用线性核函数，随机种子42（也可以自己调试、体验不同参数的作用，选择更好的值，注释明确即可）\n",
    "3. 使用 Accuracy、Recall、F1 Score、Confusion Matri 这四个评估指标来评估实验效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 加载数据，划分数据集\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 将数据集划分为训练集和测试集，测试集比例0.2，随机种子42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 特征缩放（可选，但推荐）\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. 创建并训练 SVM 模型，使用线性核函数，随机种子42\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. 预测\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# 4. 评估模型\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')  # 多分类问题使用macro平均\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM 基本概念  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将实例的特征向量（以二维为例）映射为空间中的一些点，如下图的实心点和空心点，它们属于不同的两类。SVM 的目的就是想要画出一条线，以“最好地”区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。\n",
    "\n",
    "支持向量机（support vector machines，SVM）是一种二分类模型，它将实例的特征向量映射为空间中的一些点，SVM 的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。SVM 适合中小型数据样本、非线性、高维的分类问题。\n",
    "\n",
    "SVM 是有监督的学习模型，就是说我们需要先对数据打上标签，之后通过求解最大分类间隔来求解二分类问题，而对于多分类问题，可以组合多个 SVM 分类器来处理。\n",
    "\n",
    "![image](https://github.com/X-lab2017/OpenTEA101/assets/115639837/f85c6a50-aca5-44d9-85e0-d1a6b4b57280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 题目二、以新闻数据分类为例 学习朴素贝叶斯\n",
    "1. 导入库与数据集，数据集的导入方式：from sklearn.datasets import fetch_20newsgroups\n",
    "2. 查看类别标签、数据集的描述、数据样本\n",
    "3. 将文本数据转换为词袋模型\n",
    "4. 将数据集分为训练集和测试集，训练集比例0.2，随机种子42\n",
    "5. 创建并训练朴素贝叶斯分类器\n",
    "6. 使用 Accuracy、Recall、F1 Score 这三个评估指标来评估实验效果\n",
    "7. 横坐标为 Predicted，纵坐标为 Actual，画出混淆矩阵Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. 导入数据集\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# 2. 查看类别标签、数据集的描述、数据样本\n",
    "print(\"Categories:\", newsgroups_data.target_names)\n",
    "print(\"Description:\\n\", newsgroups_data.DESCR[:500])  # 只打印描述的前500个字符\n",
    "print(\"Sample data:\\n\", newsgroups_data.data[0][:200])  # 只打印样本数据的前200个字符\n",
    "\n",
    "# 3. 将文本数据转换为词袋模型\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, min_df=5)\n",
    "X = vectorizer.fit_transform(newsgroups_data.data)\n",
    "y = newsgroups_data.target\n",
    "\n",
    "# 4. 将数据集分为训练集和测试集，训练集比例0.2，随机种子42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "# 5. 创建并训练朴素贝叶斯分类器\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. 使用 Accuracy、Recall、F1 Score 这三个评估指标来评估实验效果\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# 7. 画出混淆矩阵\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    xticklabels=newsgroups_data.target_names,\n",
    "    yticklabels=newsgroups_data.target_names,\n",
    "    cmap='YlGnBu'\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 朴素贝叶斯基本概念  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯（Naive Bayes）是一种基于贝叶斯定理的统计学分类方法。它被广泛应用于机器学习和数据挖掘领域，特别是在文本分类和垃圾邮件过滤等任务中取得了很好的效果。\n",
    "\n",
    "![image](https://github.com/hypertrons/hypertrons-crx/assets/50283262/6b01a8df-b83d-48a2-a13f-9dfe60335f9e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类是一种无监督学习的方法，旨在将数据集中的样本分组（或簇）成相似的集合，使得同一组内的样本相互之间更相似，而不同组之间的样本更不相似。\n",
    "\n",
    "聚类是发现数据内在结构的一种方法，它能够帮助我们理解数据的组织、发现隐藏的模式以及从数据中提取有用的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 题目三、以鸢尾花数据集为例 学习k-means聚类\n",
    "1. 导入数据集和聚类库\n",
    "2. 使用k-means聚类，将数据分为3个簇，设置随机种子为0\n",
    "3. PCA 降维到2维空间后，输出可视化结果\n",
    "4. 尝试先降维，再聚类，再输出可视化结果，比较两次的不同\n",
    "5. 使用轮廓系数比较聚类效果\n",
    "6. 绘制轮廓系数与聚类数的关系图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 1. 导入数据集和聚类库\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. 使用k-means聚类，将数据分为3个簇，设置随机种子为0\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(\"Cluster labels for each point (original data):\", labels)\n",
    "\n",
    "# 3. PCA 降维到2维空间后，输出可视化结果\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k')\n",
    "plt.title('K-means Clustering on Iris Dataset (PCA-reduced data)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# 4. 尝试先降维，再聚类，再输出可视化结果\n",
    "pca_for_clustering = PCA(n_components=2)\n",
    "X_pca_for_clustering = pca_for_clustering.fit_transform(X)\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans_pca.fit(X_pca_for_clustering)\n",
    "labels_pca = kmeans_pca.labels_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_for_clustering[:, 0], X_pca_for_clustering[:, 1], c=labels_pca, cmap='viridis', marker='o', edgecolor='k')\n",
    "plt.title('K-means Clustering on Iris Dataset (PCA before clustering)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# 5. 使用轮廓系数比较聚类效果\n",
    "silhouette_avg_original = silhouette_score(X, labels)\n",
    "silhouette_avg_pca = silhouette_score(X_pca_for_clustering, labels_pca)\n",
    "\n",
    "print(f\"Silhouette Score (original data clustering): {silhouette_avg_original}\")\n",
    "print(f\"Silhouette Score (PCA-reduced data clustering): {silhouette_avg_pca}\")\n",
    "\n",
    "# 6. 绘制轮廓系数与聚类数的关系图\n",
    "range_n_clusters = list(range(2, 11))\n",
    "silhouette_scores_original = []\n",
    "silhouette_scores_pca = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # 原始数据\n",
    "    kmeans_temp = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans_temp.fit(X)\n",
    "    silhouette_scores_original.append(silhouette_score(X, kmeans_temp.labels_))\n",
    "    \n",
    "    # PCA降维数据\n",
    "    kmeans_pca_temp = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans_pca_temp.fit(X_pca_for_clustering)\n",
    "    silhouette_scores_pca.append(silhouette_score(X_pca_for_clustering, kmeans_pca_temp.labels_))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_n_clusters, silhouette_scores_original, marker='o')\n",
    "plt.title('Silhouette Scores (Original Data)')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_n_clusters, silhouette_scores_pca, marker='o')\n",
    "plt.title('Silhouette Scores (PCA-reduced Data)')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**轮廓系数（Silhouette Coefficient）** 是一种用于度量数据点与其自身簇内数据的相似度与与最近的相邻簇的数据点的不相似度的指标。\n",
    "\n",
    "1. 对于每个样本，计算它与同簇内所有其他点的平均距离（称为簇内平均距离，a）。\n",
    "\n",
    "2. 对于每个样本，计算它与最近的不同簇内所有点的平均距离（称为簇间平均距离，b）。\n",
    "\n",
    "3. 计算轮廓系数（S）：\n",
    "\n",
    "​![image](https://github.com/hypertrons/hypertrons-crx/assets/50283262/6f26e49d-f738-4f41-998b-75e4870829f2)\n",
    "\n",
    "轮廓系数的取值范围在[-1, 1]之间：\n",
    "\n",
    "- 如果 S 接近1，表示样本与自身簇内的其他样本相似度高，与其他簇内的样本不相似，聚类效果好。\n",
    "- 如果 S 接近-1，表示样本与自身簇内的其他样本相似度低，与其他簇内的样本相似度高，聚类效果差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
